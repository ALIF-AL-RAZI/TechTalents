{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import re\n","import string\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import joblib\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Install required packages\n","!pip install langdetect\n","!pip install nltk\n","!pip install sbnltk\n","!pip install textblob\n","!pip install imblearn\n","\n","# Import text processing libraries\n","import pandas as pd\n","import re\n","import string\n","from langdetect import detect\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","from sbnltk.Stemmer import stemmerOP\n","from sbnltk.Preprocessor import preprocessor\n","from sbnltk.Tokenizer import wordTokenizer, sentenceTokenizer\n","from textblob import TextBlob\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.pipeline import Pipeline\n","from collections import Counter\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt_tab')\n","\n","# Load data\n","df = pd.read_excel('/content/drive/MyDrive/TechTalentAssignment/Data & Topics.xlsx')\n","df.drop_duplicates(inplace=True)\n","print(f\"Dataset shape after removing duplicates: {df.shape}\")\n","\n","# Define text processing functions\n","# Emoji removal pattern\n","emoji_pattern = re.compile(\n","    \"[\"\n","    u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n","    u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n","    u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map\n","    u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n","    u\"\\U00002700-\\U000027BF\"  # Dingbats\n","    u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols\n","    u\"\\U0001FA70-\\U0001FAFF\"  # Extended-A (ðŸ«µ)\n","    u\"\\U00002600-\\U000026FF\"  # Misc symbols\n","    u\"\\U0001F000-\\U0001F02F\"  # Mahjong\n","    u\"\\U0000203C\"             # Double exclamation\n","    u\"\\uFE0F\"                 # Variation Selector-16\n","    \"]+\",\n","    flags=re.UNICODE\n",")\n","\n","# Custom punctuation (Bangla, Urdu, Arabic)\n","custom_punct = 'à¥¤à¥¥''\"\"â€¦ØŒØ›ØŸ'\n","all_punct = string.punctuation + custom_punct\n","\n","# Load stopwords\n","en_stop = set(stopwords.words('english'))\n","\n","# Initialize processors\n","en_lem = WordNetLemmatizer()\n","bn_preprocessor = preprocessor()\n","bn_word_tokenizer = wordTokenizer()\n","bn_stemmer = stemmerOP()\n","\n","# Detect Bengali characters\n","def is_bengali(word):\n","    return bool(re.search(r'[\\u0980-\\u09FF]', word))\n","\n","# Clean and preprocess text\n","def preprocess_text(text):\n","    text = str(text)\n","\n","    # Remove URLs, hashtags, mentions\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = re.sub(r\"#\\w+\", \"\", text)\n","    text = re.sub(r\"@\\w+\", \"\", text)\n","\n","    # Remove emojis\n","    text = emoji_pattern.sub('', text)\n","\n","    # Remove punctuations\n","    text = re.sub(f\"[{re.escape(all_punct)}]\", \"\", text)\n","\n","    # Normalize space\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    try:\n","        lang = detect(text)\n","    except:\n","        lang = 'en'\n","\n","    # For Bengali text, use SBNLTK tokenizer\n","    if (lang == 'bn'):\n","        try:\n","            text = bn_preprocessor.word_normalize(text)\n","            # Tokenize into words\n","            tokens = bn_word_tokenizer.basic_tokenizer(text)\n","\n","            # Apply stemming to each token\n","            tokens = [bn_stemmer.stemWord(t) for t in tokens]\n","            # Apply dust remove to each token\n","            tokens = [bn_preprocessor.dust_removal(t) for t in tokens]\n","        except:\n","            # Fallback to NLTK if SBNLTK fails\n","            tokens = word_tokenize(text)\n","            tokens = [t.lower() for t in tokens if t.isalpha()]\n","    else:\n","        # For English, use NLTK with TextBlob correction\n","        try:\n","            textBlb = TextBlob(text)\n","            textBlb = textBlb.correct()\n","            text = textBlb.string\n","        except:\n","            # If TextBlob correction fails\n","            pass\n","\n","        # Tokenize into words\n","        tokens = word_tokenize(text)\n","        tokens = [t.lower() for t in tokens if t.isalpha()]\n","\n","    clean_tokens = []\n","    for token in tokens:\n","        if is_bengali(token):\n","            # Bengali tokens are already preprocessed and stemmed\n","            clean_tokens.append(token)\n","        else:\n","            if token not in en_stop:\n","                lemma = en_lem.lemmatize(token)\n","                clean_tokens.append(lemma)\n","\n","    return ' '.join(clean_tokens)  # Return as space-separated string for TF-IDF\n","\n","# Apply preprocessing to data\n","print(\"Applying text preprocessing...\")\n","df['clean_context'] = df['Context'].apply(preprocess_text)\n","\n","# Remove empty documents after cleaning\n","df = df[df['clean_context'].str.strip() != '']\n","print(f\"Dataset shape after removing empty documents: {df.shape}\")\n","\n","# Check class distribution\n","X = df['clean_context']\n","y = df['Training Topic']\n","\n","print(\"\\nChecking original class distribution...\")\n","class_counts = y.value_counts()\n","print(class_counts)\n","\n","# Filter out classes with only one sample\n","print(\"\\nRemoving classes with only one sample...\")\n","valid_classes = class_counts[class_counts >= 2].index\n","filtered_df = df[df['Training Topic'].isin(valid_classes)]\n","print(f\"Original dataset size: {len(df)}\")\n","print(f\"Filtered dataset size: {len(filtered_df)}\")\n","\n","# Get the new X and y from filtered dataset\n","X = filtered_df['clean_context']\n","y = filtered_df['Training Topic']\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9YnCBARL6Ogy","executionInfo":{"status":"error","timestamp":1746289173135,"user_tz":-360,"elapsed":537449,"user":{"displayName":"Jrd Aar","userId":"17453757585559135588"}},"outputId":"f1cff2dd-4ffd-4acd-b0ce-78b2b64afd22"},"id":"9YnCBARL6Ogy","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=8be502c944522b8ea42854d883dcaaa59f430db4d263f1ef74664af4d4316360\n","  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Collecting sbnltk\n","  Downloading sbnltk-2.0.2.tar.gz (28 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: gdown>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from sbnltk) (5.2.0)\n","Collecting google_trans_new>=1.1.9 (from sbnltk)\n","  Downloading google_trans_new-1.1.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: pandas>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from sbnltk) (2.2.2)\n","Requirement already satisfied: transformers>=4.3.2 in /usr/local/lib/python3.11/dist-packages (from sbnltk) (4.51.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from sbnltk) (2.6.0+cu124)\n","Requirement already satisfied: tensorflow>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from sbnltk) (2.18.0)\n","Collecting sklearn_crfsuite>=0.3.6 (from sbnltk)\n","  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n","Collecting pytorch_pretrained_bert>=0.6.2 (from sbnltk)\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl.metadata (86 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (from sbnltk) (3.4.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.12.2->sbnltk) (4.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.12.2->sbnltk) (3.18.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown>=3.12.2->sbnltk) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=3.12.2->sbnltk) (4.67.1)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.2->sbnltk) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.2->sbnltk) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.2->sbnltk) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.2->sbnltk) (2025.2)\n","Collecting boto3 (from pytorch_pretrained_bert>=0.6.2->sbnltk)\n","  Downloading boto3-1.38.8-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from pytorch_pretrained_bert>=0.6.2->sbnltk) (2024.11.6)\n","Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite>=0.3.6->sbnltk)\n","  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite>=0.3.6->sbnltk) (1.6.1)\n","Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite>=0.3.6->sbnltk) (0.9.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (5.29.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (3.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (4.13.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (3.13.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.4.1->sbnltk) (0.37.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->sbnltk) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->sbnltk) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->sbnltk) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->sbnltk)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->sbnltk)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->sbnltk)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->sbnltk)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->sbnltk)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->sbnltk)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->sbnltk)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->sbnltk)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->sbnltk)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->sbnltk) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->sbnltk) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sbnltk) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->sbnltk)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->sbnltk) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->sbnltk) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->sbnltk) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.3.2->sbnltk) (0.30.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.3.2->sbnltk) (6.0.2)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.3.2->sbnltk) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.3.2->sbnltk) (0.5.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->sbnltk) (1.15.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->sbnltk) (11.2.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.4.1->sbnltk) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.4.1->sbnltk) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.4.1->sbnltk) (0.0.9)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.4.1->sbnltk) (0.15.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.12.2->sbnltk) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.12.2->sbnltk) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.12.2->sbnltk) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.12.2->sbnltk) (2025.4.26)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite>=0.3.6->sbnltk) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite>=0.3.6->sbnltk) (3.6.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.4.1->sbnltk) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.4.1->sbnltk) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.4.1->sbnltk) (3.1.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.12.2->sbnltk) (2.7)\n","Collecting botocore<1.39.0,>=1.38.8 (from boto3->pytorch_pretrained_bert>=0.6.2->sbnltk)\n","  Downloading botocore-1.38.8-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert>=0.6.2->sbnltk)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.13.0,>=0.12.0 (from boto3->pytorch_pretrained_bert>=0.6.2->sbnltk)\n","  Downloading s3transfer-0.12.0-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->sbnltk) (3.0.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.12.2->sbnltk) (1.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.4.1->sbnltk) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.4.1->sbnltk) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.4.1->sbnltk) (0.1.2)\n","Downloading google_trans_new-1.1.9-py3-none-any.whl (9.2 kB)\n","Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.38.8-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.38.8-py3-none-any.whl (13.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.12.0-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: sbnltk\n","  Building wheel for sbnltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sbnltk: filename=sbnltk-2.0.2-py3-none-any.whl size=31768 sha256=ca4b7e79c6ca9000c8f3429e9b84ee552c176c1aa4ff0e8d5713781317996e7f\n","  Stored in directory: /root/.cache/pip/wheels/09/6b/83/be658ebd428c43ff9194c2cec3d7f9ed0db2bd1393f5a43477\n","Successfully built sbnltk\n","Installing collected packages: google_trans_new, python-crfsuite, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, sklearn_crfsuite, s3transfer, nvidia-cusolver-cu12, boto3, pytorch_pretrained_bert, sbnltk\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed boto3-1.38.8 botocore-1.38.8 google_trans_new-1.1.9 jmespath-1.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-crfsuite-0.9.11 pytorch_pretrained_bert-0.6.2 s3transfer-0.12.0 sbnltk-2.0.2 sklearn_crfsuite-0.5.0\n","Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n","Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n","Collecting imblearn\n","  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n","Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (2.0.2)\n","Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.15.2)\n","Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n","Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n","Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n","Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (3.6.0)\n","Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n","Installing collected packages: imblearn\n","Successfully installed imblearn-0.0\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=142XvJg9xdpgzuYD31Y4pm-ZVdMaWmtuq\n","To: /usr/local/lib/python3.11/dist-packages/sbnltk/dataset/download_link.txt\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.66k/1.66k [00:00<00:00, 4.39MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1UIR2arP_6Fm0MqJVbcG2Q9iLCnjQriDB\n","To: /usr/local/lib/python3.11/dist-packages/sbnltk/dataset/bangla_word_list.txt\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.4M/15.4M [00:00<00:00, 148MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1e4x5FQrdGyEfBBzQ_DOJ9wBxZZ1F7bx7\n","To: /usr/local/lib/python3.11/dist-packages/sbnltk/dataset/stopword_list.txt\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.77k/4.77k [00:00<00:00, 8.82MB/s]\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Dataset shape after removing duplicates: (551, 3)\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1ohRYMGAeq4RZTa3W-7DXEsaUCaVNIxgO\n","To: /usr/local/lib/python3.11/dist-packages/sbnltk/dataset/rootword_list.txt\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.00M/2.00M [00:00<00:00, 138MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1oBEK2DUs6Jt155AmMyc9565dHJJFZd_N\n","To: /usr/local/lib/python3.11/dist-packages/sbnltk/dataset/ner_static.txt\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.2M/20.2M [00:00<00:00, 44.0MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Applying text preprocessing...\n","Dataset shape after removing empty documents: (544, 4)\n","\n","Checking original class distribution...\n","Training Topic\n","Mob Justice                   124\n","Law and Order                  66\n","Politics                       59\n","Islamic Fundamentalism         45\n","International affairs          34\n","Religion                       33\n","Corruption                     30\n","National Defence               27\n","Diplomacy                      24\n","Governance & Policy Reform     20\n","Women Rights                   19\n","Sports                         15\n","Terrorism                      12\n","Election                       11\n","Culture & Lifestyle            10\n","Trade & Commodity Price         6\n","Education                       4\n","Environment                     2\n","Natural Disaster                2\n","Religious Conflict              1\n","Name: count, dtype: int64\n","\n","Removing classes with only one sample...\n","Original dataset size: 544\n","Filtered dataset size: 543\n","\n","Performing train-test split...\n","\n","Applying TF-IDF vectorization...\n","\n","Applying balanced sampling strategy...\n"]},{"output_type":"error","ename":"ValueError","evalue":"With under-sampling methods, the number of samples in a class should be less or equal to the original number of samples. Originally, there is 27 samples and 30 samples are asked.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6bcbed8a1eec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m ])\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m \u001b[0mX_train_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;31m# Show the class distribution after resampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \"\"\"\n\u001b[1;32m    725\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit_resample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 )\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_resample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 X, y, fitted_transformer = fit_resample_one_cached(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_resample_one\u001b[0;34m(sampler, X, y, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_resample_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m         \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_resample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \"\"\"\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36mcheck_sampling_strategy\u001b[0;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         return OrderedDict(\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sampling_strategy_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         )\n\u001b[1;32m    559\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36m_sampling_strategy_dict\u001b[0;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclass_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampling_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtarget_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    347\u001b[0m                     \u001b[0;34m\"With under-sampling methods, the number of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                     \u001b[0;34m\" samples in a class should be less or equal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: With under-sampling methods, the number of samples in a class should be less or equal to the original number of samples. Originally, there is 27 samples and 30 samples are asked."]}]},{"cell_type":"code","source":["# Define a target for balancing classes\n","# For larger classes (>30 samples): undersample to 30\n","# For medium classes (10-30 samples): keep as is\n","# For smaller classes (<10 samples): oversample to 10\n","def get_sampling_strategy(class_counts): # Pass class_counts as an argument\n","    strategy = {}\n","    for class_name, count in class_counts.items():\n","        if count > 30:  # Undersample large classes\n","            strategy[class_name] = 30\n","        elif count < 10 and count >= 2:  # Oversample small classes\n","            strategy[class_name] = 10\n","        # Medium classes keep their original count\n","    return strategy"],"metadata":{"id":"TVUPKt-59LVv","executionInfo":{"status":"ok","timestamp":1746289657364,"user_tz":-360,"elapsed":6,"user":{"displayName":"Jrd Aar","userId":"17453757585559135588"}}},"id":"TVUPKt-59LVv","execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Now perform the train-test split\n","print(\"\\nPerforming train-test split...\")\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# TF-IDF Vectorization\n","print(\"\\nApplying TF-IDF vectorization...\")\n","tfidf_vectorizer = TfidfVectorizer(\n","    max_features=5000,  # Limit features to prevent overfitting\n","    min_df=2,           # Ignore terms that appear in less than 2 documents\n","    max_df=0.8,         # Ignore terms that appear in more than 80% of documents\n","    ngram_range=(1, 2)  # Use both unigrams and bigrams\n",")\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# Apply combined undersampling and oversampling\n","print(\"\\nApplying balanced sampling strategy...\")\n","# Get updated class counts from y_train\n","train_class_counts = y_train.value_counts()\n","undersampler = RandomUnderSampler(\n","    sampling_strategy=get_sampling_strategy(train_class_counts), # Use updated counts\n","    random_state=42\n",")\n","oversampler = SMOTE(\n","    sampling_strategy='auto',  # Will oversample all minority classes to match the majority\n","    random_state=42,\n","    k_neighbors=5  # Lower k_neighbors for small classes\n",")\n","\n","# Apply undersampling then oversampling in a pipeline\n","sampling_pipeline = Pipeline([\n","    ('undersample', undersampler),\n","    ('oversample', oversampler)\n","])\n","\n","X_train_resampled, y_train_resampled = sampling_pipeline.fit_resample(X_train_tfidf, y_train)\n","\n","# Show the class distribution after resampling\n","print(\"\\nClass distribution after resampling:\")\n","resampled_class_counts = pd.Series(y_train_resampled).value_counts()\n","print(resampled_class_counts)\n","\n","# Define and train multiple models\n","models = {\n","    'Logistic Regression': LogisticRegression(\n","        C=1.0,\n","        max_iter=1000,\n","        multi_class='multinomial',\n","        solver='lbfgs',\n","        random_state=42\n","    ),\n","    'SVM': SVC(\n","        C=1.0,\n","        kernel='linear',\n","        probability=True,\n","        random_state=42\n","    ),\n","    'Random Forest': RandomForestClassifier(\n","        n_estimators=100,\n","        max_depth=None,\n","        min_samples_split=2,\n","        random_state=42\n","    ),\n","    'Naive Bayes': MultinomialNB(alpha=0.1)\n","}\n","\n","# Dictionary to store model results\n","model_results = {}\n","\n","for name, model in models.items():\n","    print(f\"\\nTraining {name} model...\")\n","    model.fit(X_train_resampled, y_train_resampled)\n","\n","    # Make predictions\n","    y_train_pred = model.predict(X_train_tfidf)\n","    y_test_pred = model.predict(X_test_tfidf)\n","\n","    # Calculate metrics\n","    train_accuracy = accuracy_score(y_train, y_train_pred)\n","    test_accuracy = accuracy_score(y_test, y_test_pred)\n","\n","    print(f\"Training accuracy: {train_accuracy:.4f}\")\n","    print(f\"Test accuracy: {test_accuracy:.4f}\")\n","\n","    # Store results\n","    model_results[name] = {\n","        'model': model,\n","        'train_accuracy': train_accuracy,\n","        'test_accuracy': test_accuracy,\n","        'train_predictions': y_train_pred,\n","        'test_predictions': y_test_pred,\n","        'test_report': classification_report(y_test, y_test_pred, output_dict=True)\n","    }\n","\n","    # Print classification report for test set\n","    print(\"\\nClassification Report (Test Set):\")\n","    print(classification_report(y_test, y_test_pred))\n","\n","    # Generate confusion matrix\n","    plt.figure(figsize=(12, 10))\n","    cm_test = confusion_matrix(y_test, y_test_pred)\n","    cm_test_normalized = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n","    sns.heatmap(cm_test_normalized, annot=True, fmt='.2f', cmap='Greens',\n","                xticklabels=model.classes_, yticklabels=model.classes_)\n","    plt.title(f'Normalized Confusion Matrix - {name} (Test Set)')\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.xticks(rotation=90)\n","    plt.yticks(rotation=0)\n","    plt.tight_layout()\n","    plt.savefig(f'confusion_matrix_{name.replace(\" \", \"_\").lower()}.png')\n","    plt.close()\n","\n","# Compare model performances\n","print(\"\\nModel Performance Comparison:\")\n","model_comparison = pd.DataFrame({\n","    name: {\n","        'Train Accuracy': results['train_accuracy'],\n","        'Test Accuracy': results['test_accuracy'],\n","        'Weighted F1-Score': results['test_report']['weighted avg']['f1-score']\n","    } for name, results in model_results.items()\n","}).T\n","\n","print(model_comparison)\n","\n","# Visualize model comparison\n","plt.figure(figsize=(12, 6))\n","model_comparison[['Train Accuracy', 'Test Accuracy', 'Weighted F1-Score']].plot(kind='bar')\n","plt.title('Model Performance Comparison')\n","plt.ylim(0, 1.0)\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.savefig('model_comparison.png')\n","plt.show()\n","\n","# Find the best model based on test accuracy\n","best_model_name = model_comparison['Test Accuracy'].idxmax()\n","best_model = model_results[best_model_name]['model']\n","print(f\"\\nBest performing model: {best_model_name}\")\n","\n","# Save the best model and vectorizer\n","print(f\"\\nSaving the best model ({best_model_name}) and vectorizer...\")\n","joblib.dump(best_model, f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n","joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n","print(\"Model and vectorizer saved successfully!\")\n","\n","# Feature importance analysis for the best model\n","print(\"\\nTop features for each class (using the best model):\")\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","\n","# Create a DataFrame to store feature importance for visualization\n","feature_importance_data = []\n","\n","# Different feature importance extraction based on model type\n","if best_model_name == 'Logistic Regression' or best_model_name == 'SVM':\n","    if hasattr(best_model, 'coef_'):\n","        for i, class_label in enumerate(best_model.classes_):\n","            # Get top features with highest coefficients for this class\n","            top_features_idx = np.argsort(best_model.coef_[i])[-15:]\n","            top_features = [feature_names[idx] for idx in top_features_idx]\n","            top_coefficients = [best_model.coef_[i][idx] for idx in top_features_idx]\n","\n","            # Print top features for this class\n","            print(f\"Class '{class_label}': {', '.join(top_features)}\")\n","\n","            # Add to the feature importance data\n","            for feature, coef in zip(top_features, top_coefficients):\n","                feature_importance_data.append({\n","                    'Class': class_label,\n","                    'Feature': feature,\n","                    'Importance': coef\n","                })\n","elif best_model_name == 'Random Forest':\n","    # For Random Forest, use feature importances\n","    importances = best_model.feature_importances_\n","    indices = np.argsort(importances)[-30:]  # Get top 30 features\n","\n","    print(\"Global feature importance:\")\n","    for idx in reversed(indices):\n","        print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n","\n","    # Random Forest doesn't have class-specific feature importance by default\n","    # But we can visualize top global features\n","    plt.figure(figsize=(10, 8))\n","    plt.barh(range(len(indices)), importances[indices], align='center')\n","    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n","    plt.title('Random Forest Feature Importance')\n","    plt.tight_layout()\n","    plt.savefig('rf_feature_importance.png')\n","    plt.show()\n","\n","# If we have feature importance data, visualize it\n","if feature_importance_data and (best_model_name == 'Logistic Regression' or best_model_name == 'SVM'):\n","    # Visualize top features for the most prevalent classes\n","    top_classes = class_counts.index[:5]  # Top 5 classes\n","    plt.figure(figsize=(15, 10))\n","    for i, class_name in enumerate(top_classes):\n","        class_data = [item for item in feature_importance_data if item['Class'] == class_name]\n","        class_data = sorted(class_data, key=lambda x: x['Importance'], reverse=True)[:10]\n","\n","        plt.subplot(2, 3, i+1)\n","        features = [item['Feature'] for item in class_data]\n","        importances = [item['Importance'] for item in class_data]\n","\n","        y_pos = np.arange(len(features))\n","        plt.barh(y_pos, importances, align='center')\n","        plt.yticks(y_pos, features)\n","        plt.xlabel('Coefficient Value')\n","        plt.title(f\"Top Features: {class_name}\")\n","\n","    plt.tight_layout()\n","    plt.savefig('top_features_best_model.png')\n","    plt.show()\n","\n","# Function for prediction with new text using the best model\n","def predict_topic(text, model=best_model, vectorizer=tfidf_vectorizer):\n","    # Preprocess the text\n","    clean_text = preprocess_text(text)\n","    # Vectorize the text\n","    text_tfidf = vectorizer.transform([clean_text])\n","    # Predict the topic\n","    prediction = model.predict(text_tfidf)[0]\n","    # Get prediction probabilities\n","    proba = model.predict_proba(text_tfidf)[0] if hasattr(model, 'predict_proba') else None\n","\n","    if proba is not None:\n","        # Find top 3 classes\n","        top3_idx = proba.argsort()[-3:][::-1]\n","        top3_classes = [model.classes_[i] for i in top3_idx]\n","        top3_probs = [proba[i] for i in top3_idx]\n","\n","        return {\n","            'predicted_topic': prediction,\n","            'top3_topics': list(zip(top3_classes, top3_probs))\n","        }\n","    else:\n","        return {\n","            'predicted_topic': prediction,\n","            'top3_topics': []\n","        }\n","\n","# Demo prediction with sample texts\n","sample_texts = [\n","    \"Police arrested three individuals involved in corruption at the ministry\",\n","    \"The president announced new policies to improve governance\",\n","    \"Mob beats thief to death in local market\"\n","]\n","\n","print(\"\\nSample predictions with the best model:\")\n","for text in sample_texts:\n","    prediction_result = predict_topic(text)\n","    print(f\"\\nText: '{text}'\")\n","    print(f\"Predicted Topic: {prediction_result['predicted_topic']}\")\n","    if prediction_result['top3_topics']:\n","        print(\"Top 3 predictions:\")\n","        for topic, prob in prediction_result['top3_topics']:\n","            print(f\"  {topic}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"STH-wZ3U9JGQ","executionInfo":{"status":"error","timestamp":1746289660696,"user_tz":-360,"elapsed":184,"user":{"displayName":"Jrd Aar","userId":"17453757585559135588"}},"outputId":"15d22bbf-97d5-431a-f63c-4b6f1c82810a"},"id":"STH-wZ3U9JGQ","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Performing train-test split...\n","\n","Applying TF-IDF vectorization...\n","\n","Applying balanced sampling strategy...\n"]},{"output_type":"error","ename":"ValueError","evalue":"With under-sampling methods, the number of samples in a class should be less or equal to the original number of samples. Originally, there is 9 samples and 10 samples are asked.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-b9eec1eaaeee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m ])\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mX_train_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Show the class distribution after resampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \"\"\"\n\u001b[1;32m    725\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit_resample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 )\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_resample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 X, y, fitted_transformer = fit_resample_one_cached(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_resample_one\u001b[0;34m(sampler, X, y, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_resample_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m         \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_resample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \"\"\"\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36mcheck_sampling_strategy\u001b[0;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         return OrderedDict(\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sampling_strategy_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         )\n\u001b[1;32m    559\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36m_sampling_strategy_dict\u001b[0;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclass_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampling_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtarget_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    347\u001b[0m                     \u001b[0;34m\"With under-sampling methods, the number of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                     \u001b[0;34m\" samples in a class should be less or equal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: With under-sampling methods, the number of samples in a class should be less or equal to the original number of samples. Originally, there is 9 samples and 10 samples are asked."]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":5}